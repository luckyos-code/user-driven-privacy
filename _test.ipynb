{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55f9a79-79ec-44d5-8f37-06c815882ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c477b9f4-d2fc-4354-bc07-e8d982f6dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(path):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a directory, combine them into a dataset,\n",
    "    check for duplicates, and return statistics.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the directory containing CSV files\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (directory_name, original_length, cleaned_length, duplicate_count)\n",
    "    \"\"\"\n",
    "    # Extract directory name for reporting\n",
    "    leaf = os.path.basename(os.path.normpath(path))\n",
    "    parent = os.path.basename(os.path.dirname(os.path.normpath(path)))\n",
    "    directory_name = f\"{parent}/{leaf}\"\n",
    "    \n",
    "    # Find all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {path}\")\n",
    "        return directory_name, 0, 0, 0\n",
    "    \n",
    "    # Read each CSV file and store as columns in a dataframe\n",
    "    dataframes = {}\n",
    "    for csv_file in csv_files:\n",
    "        column_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "        try:\n",
    "            # Assuming each CSV file contains a single column of data\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # If the CSV has multiple columns, use the first one\n",
    "            if len(df.columns) > 1:\n",
    "                dataframes[column_name] = df.iloc[:, 0]\n",
    "            else:\n",
    "                dataframes[column_name] = df[df.columns[0]]\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "    \n",
    "    # Create a single dataframe from all columns\n",
    "    if dataframes:\n",
    "        combined_df = pd.DataFrame(dataframes)\n",
    "    else:\n",
    "        print(f\"No valid data found in CSV files in {path}\")\n",
    "        return directory_name, 0, 0, 0\n",
    "    \n",
    "    # Get original length\n",
    "    original_length = len(combined_df)\n",
    "    print(f\"\\nDirectory: {directory_name}\")\n",
    "    print(f\"Original dataset shape: {combined_df.shape}\")\n",
    "    print(f\"Original dataset length: {original_length}\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_rows = combined_df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    cleaned_df = combined_df.drop_duplicates().reset_index(drop=True)\n",
    "    cleaned_length = len(cleaned_df)\n",
    "    print(f\"Cleaned dataset shape: {cleaned_df.shape}\")\n",
    "    print(f\"Cleaned dataset length: {cleaned_length}\")\n",
    "    print(f\"Removed {original_length - cleaned_length} duplicate rows\")\n",
    "    \n",
    "    return directory_name, original_length, cleaned_length, duplicate_rows\n",
    "\n",
    "def process_all_directories(base_path, additional_path=\"\"):\n",
    "    \"\"\"\n",
    "    Process all directories in the base path.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Base directory containing subdirectories with CSV files\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get all directories in the base path\n",
    "    directories = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    \n",
    "    if not directories:\n",
    "        print(f\"No subdirectories found in {base_path}\")\n",
    "        return\n",
    "    \n",
    "    # Process each directory\n",
    "    for directory in directories:\n",
    "        dir_path = os.path.join(base_path, directory, additional_path)\n",
    "        result = process_directory(dir_path)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SUMMARY OF RESULTS\")\n",
    "    print(\"=\"*100)\n",
    "    print(\"{:<50} {:<15} {:<15} {:<15}\".format(\n",
    "        \"Directory\", \"Original Size\", \"Cleaned Size\", \"Duplicates\"))\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for dir_name, orig_len, clean_len, dup_count in results:\n",
    "        print(\"{:<50} {:<15} {:<15} {:<15}\".format(\n",
    "            dir_name, orig_len, clean_len, dup_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e54de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CSV files found in /Users/lucas/Downloads/data/diabetes/complete_weighted_specialization/33-33-34\n",
      "\n",
      "Directory: specialization/33-33-34\n",
      "Original dataset shape: (118057, 21)\n",
      "Original dataset length: 118057\n",
      "Number of duplicate rows: 23682\n",
      "Cleaned dataset shape: (94375, 21)\n",
      "Cleaned dataset length: 94375\n",
      "Removed 23682 duplicate rows\n",
      "\n",
      "Directory: generalization/33-33-34\n",
      "Original dataset shape: (56553, 2)\n",
      "Original dataset length: 56553\n",
      "Number of duplicate rows: 0\n",
      "Cleaned dataset shape: (56553, 2)\n",
      "Cleaned dataset length: 56553\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "Directory: forced_generalization/33-33-34\n",
      "Original dataset shape: (70692, 1)\n",
      "Original dataset length: 70692\n",
      "Number of duplicate rows: 0\n",
      "Cleaned dataset shape: (70692, 1)\n",
      "Cleaned dataset length: 70692\n",
      "Removed 0 duplicate rows\n",
      "No CSV files found in /Users/lucas/Downloads/data/diabetes/complete_forced_generalization/33-33-34\n",
      "No CSV files found in /Users/lucas/Downloads/data/diabetes/complete_extended_weighted_specialization/33-33-34\n",
      "No CSV files found in /Users/lucas/Downloads/data/diabetes/complete_generalization/33-33-34\n",
      "\n",
      "Directory: extended_weighted_specialization/33-33-34\n",
      "Original dataset shape: (539547, 21)\n",
      "Original dataset length: 539547\n",
      "Number of duplicate rows: 404311\n",
      "Cleaned dataset shape: (135236, 21)\n",
      "Cleaned dataset length: 135236\n",
      "Removed 404311 duplicate rows\n",
      "\n",
      "====================================================================================================\n",
      "SUMMARY OF RESULTS\n",
      "====================================================================================================\n",
      "Directory                                          Original Size   Cleaned Size    Duplicates     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "complete_weighted_specialization/33-33-34          0               0               0              \n",
      "specialization/33-33-34                            118057          94375           23682          \n",
      "generalization/33-33-34                            56553           56553           0              \n",
      "forced_generalization/33-33-34                     70692           70692           0              \n",
      "complete_forced_generalization/33-33-34            0               0               0              \n",
      "complete_extended_weighted_specialization/33-33-34 0               0               0              \n",
      "complete_generalization/33-33-34                   0               0               0              \n",
      "extended_weighted_specialization/33-33-34          539547          135236          404311         \n"
     ]
    }
   ],
   "source": [
    "# new counts\n",
    "path = os.path.join(\"/Users/lucas/Downloads\", \"data\", \"diabetes\")\n",
    "process_all_directories(path, additional_path=\"33-33-34\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b3919-85ca-4574-aa79-799fde40abd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: complete_forced_generalization\n",
      "Original dataset shape: (45222, 1)\n",
      "Original dataset length: 45222\n",
      "Number of duplicate rows: 0\n",
      "Cleaned dataset shape: (45222, 1)\n",
      "Cleaned dataset length: 45222\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "Directory: extended_weighted_specialization\n",
      "Original dataset shape: (1489716, 13)\n",
      "Original dataset length: 1489716\n",
      "Number of duplicate rows: 1282764\n",
      "Cleaned dataset shape: (206952, 13)\n",
      "Cleaned dataset length: 206952\n",
      "Removed 1282764 duplicate rows\n",
      "\n",
      "Directory: forced_generalization\n",
      "Original dataset shape: (45222, 1)\n",
      "Original dataset length: 45222\n",
      "Number of duplicate rows: 0\n",
      "Cleaned dataset shape: (45222, 1)\n",
      "Cleaned dataset length: 45222\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "Directory: generalization\n",
      "Original dataset shape: (36177, 2)\n",
      "Original dataset length: 36177\n",
      "Number of duplicate rows: 0\n",
      "Cleaned dataset shape: (36177, 2)\n",
      "Cleaned dataset length: 36177\n",
      "Removed 0 duplicate rows\n",
      "No CSV files found in /home/sc.uni-leipzig.de/ll95wyqa/projects/user-driven-privacy/datasets/adult/.ipynb_checkpoints\n",
      "\n",
      "Directory: specialization\n",
      "Original dataset shape: (885220, 13)\n",
      "Original dataset length: 885220\n",
      "Number of duplicate rows: 708366\n",
      "Cleaned dataset shape: (176854, 13)\n",
      "Cleaned dataset length: 176854\n",
      "Removed 708366 duplicate rows\n",
      "\n",
      "Directory: complete_generalization\n",
      "Original dataset shape: (36177, 2)\n",
      "Original dataset length: 36177\n",
      "Number of duplicate rows: 0\n",
      "Cleaned dataset shape: (36177, 2)\n",
      "Cleaned dataset length: 36177\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "Directory: complete_weighted_specialization\n",
      "Original dataset shape: (1303130, 13)\n",
      "Original dataset length: 1303130\n",
      "Number of duplicate rows: 1058219\n",
      "Cleaned dataset shape: (244911, 13)\n",
      "Cleaned dataset length: 244911\n",
      "Removed 1058219 duplicate rows\n",
      "\n",
      "Directory: complete_extended_weighted_specialization\n",
      "Original dataset shape: (1966056, 13)\n",
      "Original dataset length: 1966056\n",
      "Number of duplicate rows: 1681629\n",
      "Cleaned dataset shape: (284427, 13)\n",
      "Cleaned dataset length: 284427\n",
      "Removed 1681629 duplicate rows\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS\n",
      "================================================================================\n",
      "Directory                      Original Size   Cleaned Size    Duplicates     \n",
      "--------------------------------------------------------------------------------\n",
      "complete_forced_generalization 45222           45222           0              \n",
      "extended_weighted_specialization 1489716         206952          1282764        \n",
      "forced_generalization          45222           45222           0              \n",
      "generalization                 36177           36177           0              \n",
      ".ipynb_checkpoints             0               0               0              \n",
      "specialization                 885220          176854          708366         \n",
      "complete_generalization        36177           36177           0              \n",
      "complete_weighted_specialization 1303130         244911          1058219        \n",
      "complete_extended_weighted_specialization 1966056         284427          1681629        \n"
     ]
    }
   ],
   "source": [
    "# original counts\n",
    "path = os.path.join(os.getcwd(), \"datasets\", \"adult\")\n",
    "process_all_directories(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2170d52-120a-4e4a-a00a-8d9472fcc7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (user-driven-privacy)",
   "language": "python",
   "name": "user-driven-privacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
