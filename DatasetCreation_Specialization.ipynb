{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a943897e",
   "metadata": {},
   "source": [
    "# Specialization Dataset Creation (Record-Based Approach)\n",
    "\n",
    "This notebook creates specialized datasets using the **record-based approach** from Main.py, RecordBasedFiltering.py, and FilteringHandler.py.\n",
    "\n",
    "**Note:** May take significant space and time depending on parameters and datasets - take care if running locally.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. Loads generalized data (from `data/<dataset>/generalization/<percentages>/`)\n",
    "2. For each record, generates variants by expanding generalized values\n",
    "3. Applies filtering (random, imputation, knn, or none)\n",
    "4. Saves filtered specialization data\n",
    "\n",
    "## Filtering Modes\n",
    "\n",
    "- **`random`**: Randomly select n_duplicates variants per record\n",
    "- **`imputation`**: Select best n_duplicates using profile-based scoring\n",
    "- **`knn`**: Select best n_duplicates using KNN similarity (not used due to similar results at higher compute overhead)\n",
    "- **`None`**: No filtering (n_duplicates=0 keeps only unique records, None keeps all)\n",
    "\n",
    "## Realistic Mode\n",
    "\n",
    "- **`True`**: Only use values observed in original/generalized data (realistic)\n",
    "- **`False`**: Use all possible hierarchy values (unrealistic but exhaustive)\n",
    "\n",
    "## Output\n",
    "\n",
    "Saves to: `data/<dataset>/specialization/<percentages>/specialization_<filtering_mode>_n<n_duplicates>.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7d7d1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af65f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.RecordBasedSpecialization import RecordBasedSpecialization\n",
    "from src.DatasetManager import DatasetManager\n",
    "from src.Vorverarbeitung import extract_observed_values\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48a189",
   "metadata": {},
   "source": [
    "## 2. Configure Parameters\n",
    "\n",
    "Modify these parameters to create different specialization datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16764f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "\n",
    "# Dataset to process\n",
    "dataset = 'german'  # Options: 'adult', 'german', 'diabetes', 'employment'\n",
    "\n",
    "# Percentages for specialization (format: 'X-Y-Z' where X+Y+Z=100)\n",
    "# Must match a folder that exists in data/<dataset>/generalization/\n",
    "percentages = '66-17-17'\n",
    "\n",
    "# Data directory\n",
    "data_dir = 'data'\n",
    "\n",
    "# Number of variants per record (0 = only unique, None = all variants)\n",
    "n_duplicates = 2\n",
    "\n",
    "# Filtering mode\n",
    "filtering_mode = 'imputation'  # Options: 'random', 'imputation', 'knn', None\n",
    "\n",
    "#Handle \"none\" filtering\n",
    "if filtering_mode is not None and filtering_mode.lower() == 'none':\n",
    "    filtering_mode = None\n",
    "    n_duplicates = None\n",
    "\n",
    "if n_duplicates == 0:\n",
    "    filtering_mode = None\n",
    "\n",
    "# Realistic mode (uses observed values only)\n",
    "# True = Only create variants with values observed in generalized data (REALISTIC)\n",
    "# False = Use all possible hierarchy values (UNREALISTIC but exhaustive)\n",
    "\n",
    "limit_to_observed_values = True\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset: {dataset}\")\n",
    "print(f\"  Percentages: {percentages}\")\n",
    "print(f\"  filtering_mode: {filtering_mode or 'none'}\")\n",
    "print(f\"  n_duplicates: {n_duplicates or 'unlimited (no filtering)'}\")\n",
    "print(f\"  Realistic mode: {limit_to_observed_values}\")\n",
    "print(f\"  Seed: {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f00cf",
   "metadata": {},
   "source": [
    "## 3. Create Specialization Dataset\n",
    "\n",
    "This cell runs the specialization process. Depending on the dataset size and parameters, this may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RECORD-BASED SPECIALIZATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Percentages: {percentages}\")\n",
    "print(f\"n_duplicates: {n_duplicates or 'unlimited (no filtering)'}\")\n",
    "print(f\"filtering_mode: {filtering_mode or 'none'}\")\n",
    "print(f\"Realistic mode: {limit_to_observed_values}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Load generalized data\n",
    "    train_path = os.path.join(data_dir, dataset, 'generalization', percentages, f'{dataset}_train.csv')\n",
    "    test_path = os.path.join(data_dir, dataset, 'generalization', percentages, f'{dataset}_test.csv')\n",
    "    \n",
    "    print(f\"\\nLoading generalized data...\")\n",
    "    print(f\"  Train: {train_path}\")\n",
    "    print(f\"  Test: {test_path}\")\n",
    "    \n",
    "    data_train_gen = pd.read_csv(train_path)\n",
    "    data_test_gen = pd.read_csv(test_path)\n",
    "    \n",
    "    print(f\"  Train rows: {len(data_train_gen):,}\")\n",
    "    print(f\"  Test rows: {len(data_test_gen):,}\")\n",
    "    \n",
    "    # Get dataset configuration\n",
    "    spalten_dict, spalten_list = DatasetManager.get_spalten_classes(dataset)\n",
    "    numerical_columns = DatasetManager.get_numerical_columns(dataset)\n",
    "    record_id_col = DatasetManager.get_record_id_column(dataset)\n",
    "    label_col = DatasetManager.get_label_column(dataset)\n",
    "    \n",
    "    # Extract observed values if in realistic mode\n",
    "    observed_values_dict = {}\n",
    "    if limit_to_observed_values:\n",
    "        print(f\"\\nExtracting observed values from generalized data...\")\n",
    "        observed_values_dict = extract_observed_values(dataset, data_train_gen, data_dir)\n",
    "    \n",
    "    # Create record-based processor\n",
    "    print(f\"\\nInitializing RecordBasedSpecialization...\")\n",
    "    rbs = RecordBasedSpecialization(\n",
    "        dataset_name=dataset,\n",
    "        spalten_list=spalten_list,\n",
    "        numerical_columns=numerical_columns,\n",
    "        record_id_col=record_id_col,\n",
    "        label_col=label_col,\n",
    "        observed_values_dict=observed_values_dict,\n",
    "        limit_to_observed_values=limit_to_observed_values,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Process data with single configuration\n",
    "    print(f\"\\nProcessing with record-based method...\")\n",
    "    print(f\"  This may take several minutes depending on dataset size...\")\n",
    "    \n",
    "    result_train = rbs.process_data(\n",
    "        df=data_train_gen,\n",
    "        n_duplicates=n_duplicates,\n",
    "        filtering_mode=filtering_mode,\n",
    "        original_reference_data=data_train_gen\n",
    "    )\n",
    "    \n",
    "    result_test = rbs.process_data(\n",
    "        df=data_test_gen,\n",
    "        n_duplicates=n_duplicates,\n",
    "        filtering_mode=filtering_mode,\n",
    "        original_reference_data=data_test_gen\n",
    "    )\n",
    "    \n",
    "    # Convert from Dask to Pandas\n",
    "    print(f\"\\nConverting results to Pandas DataFrames...\")\n",
    "    result_train = result_train.compute()\n",
    "    result_test = result_test.compute()\n",
    "    \n",
    "    # Combine train and test\n",
    "    result_df = pd.concat([result_train, result_test], ignore_index=True)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total rows: {len(result_df):,}\")\n",
    "    print(f\"  Train: {len(result_train):,}\")\n",
    "    print(f\"  Test: {len(result_test):,}\")\n",
    "    print(f\"Unique record_ids: {result_df[record_id_col].nunique()}\")\n",
    "    if n_duplicates:\n",
    "        avg_variants = len(result_df) / result_df[record_id_col].nunique()\n",
    "        print(f\"Avg variants per record: {avg_variants:.2f}\")\n",
    "    print(f\"Processing time: {elapsed/60:.2f} minutes ({elapsed:.1f} seconds)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUCCESS!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ERROR!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Failed to process specialization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802745aa",
   "metadata": {},
   "source": [
    "## 4. Preview Results\n",
    "\n",
    "Let's inspect one of the generated files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {result_df.shape}\")\n",
    "print(f\"\\nColumns: {list(result_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39158c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variants distribution if record_id exists\n",
    "if record_id_col in result_df.columns:\n",
    "    variants_per_record = result_df.groupby(record_id_col).size()\n",
    "    print(\"Variants per record statistics:\")\n",
    "    print(variants_per_record.describe())\n",
    "    print(f\"\\nValue counts:\")\n",
    "    print(variants_per_record.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b41517",
   "metadata": {},
   "source": [
    "## 5. Save Results\n",
    "\n",
    "Save the specialized dataset to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = os.path.join(data_dir, dataset, 'specialization', percentages)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create descriptive filename with parameters\n",
    "if filtering_mode is None:\n",
    "    filename = 'specialization_unfiltered.csv'\n",
    "else:\n",
    "    n_dup_str = n_duplicates if n_duplicates is not None else 'all'\n",
    "    filename = f'specialization_{filtering_mode}_n{n_dup_str}.csv'\n",
    "\n",
    "output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "print(f\"Saving to: {output_path}\")\n",
    "result_df.to_csv(output_path, index=False)\n",
    "\n",
    "file_size_mb = os.path.getsize(output_path) / 1e6\n",
    "\n",
    "print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "print(f\"  {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\nFiltered specialization saved to:\")\n",
    "print(\"SUCCESS!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nYou may use this file instead of running specialization + filtering.\")\n",
    "print(f\"Actual specialization workflow still creates it in memory and does not read from or write to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61323196",
   "metadata": {},
   "source": [
    "## Optional: Batch Processing\n",
    "\n",
    "Use this cell to process multiple configurations in one run. But be cautious, especially on local setups!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8421fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process multiple filtering configurations efficiently\n",
    "batch_configs = [\n",
    "    (0, None),       # Keep only unique records\n",
    "    (2, 'random'),   # Keep 2 random variants\n",
    "    (2, 'imputation'), # Keep 2 best variants by imputation\n",
    "    # Add more (n_duplicates, mode) tuples as needed\n",
    "]\n",
    "\n",
    "# Uncomment to run batch processing:\n",
    "# print(f\"\\nBatch processing {len(batch_configs)} configurations...\")\n",
    "# \n",
    "# # Load generalized data once\n",
    "# train_path = os.path.join(data_dir, dataset, 'generalization', percentages, f'{dataset}_train.csv')\n",
    "# test_path = os.path.join(data_dir, dataset, 'generalization', percentages, f'{dataset}_test.csv')\n",
    "# data_train_gen = pd.read_csv(train_path)\n",
    "# data_test_gen = pd.read_csv(test_path)\n",
    "# \n",
    "# # Create processor once\n",
    "# observed_values_dict = extract_observed_values(dataset, data_train_gen, data_dir) if limit_to_observed_values else {}\n",
    "# rbs = RecordBasedSpecialization(\n",
    "#     dataset_name=dataset,\n",
    "#     spalten_list=spalten_list,\n",
    "#     numerical_columns=numerical_columns,\n",
    "#     record_id_col=record_id_col,\n",
    "#     label_col=label_col,\n",
    "#     observed_values_dict=observed_values_dict,\n",
    "#     limit_to_observed_values=limit_to_observed_values,\n",
    "#     seed=seed\n",
    "# )\n",
    "# \n",
    "# # Process all configs in one batch (optimized - generates once per mode)\n",
    "# print(\"Processing train data...\")\n",
    "# train_results = rbs.process_data_batch(data_train_gen, batch_configs, data_train_gen)\n",
    "# print(\"Processing test data...\")\n",
    "# test_results = rbs.process_data_batch(data_test_gen, batch_configs, data_test_gen)\n",
    "# \n",
    "# # Save each configuration\n",
    "# for (n_dup, mode) in batch_configs:\n",
    "#     print(f\"\\nSaving config: n_duplicates={n_dup}, mode={mode}\")\n",
    "#     \n",
    "#     # Combine train and test\n",
    "#     combined = pd.concat([\n",
    "#         train_results[(n_dup, mode)].compute(),\n",
    "#         test_results[(n_dup, mode)].compute()\n",
    "#     ], ignore_index=True)\n",
    "#     \n",
    "#     # Create filename\n",
    "#     output_dir = os.path.join(data_dir, dataset, 'specialization', percentages)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     \n",
    "#     if mode is None:\n",
    "#         filename = f'specialization_unique.csv' if n_dup == 0 else 'specialization_unfiltered.csv'\n",
    "#     else:\n",
    "#         filename = f'specialization_{mode}_n{n_dup}.csv'\n",
    "#     \n",
    "#     output_path = os.path.join(output_dir, filename)\n",
    "#     combined.to_csv(output_path, index=False)\n",
    "#     \n",
    "#     file_size_mb = os.path.getsize(output_path) / 1e6\n",
    "#     print(f\"  Saved: {output_path} ({file_size_mb:.1f} MB, {len(combined):,} rows)\")\n",
    "\n",
    "print(\"Batch processing cell ready (currently commented out)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "requirements-local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
